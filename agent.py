from typing import TypedDict, Literal
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from tools import *
from tools import AVAILABLE_TOOLS
import os

# Load environment variables from .env file
load_dotenv()

"""
Agent-driven todo executor - Implementation with LangGraph

This module implements a LangGraph AI agent that takes a high-level goal, generates a todo list,
and executes tasks autonomously or with user confirmation.

Key Components:
- Task and AgentState: TypedDict schemas for state management. AgentState is the main state object, Task is a single task in the todo list.
- Node functions: 
    - generate_todos() generates the to-do list based on the user query.
    - display_and_wait_for_approval() shows the todo list in the terminal and waits for user approval in confirm mode.
    - select_next_task() selects the next pending task from the todo list.
    - execute_task() executes the selected task using an LLM with access to defined tools.
    - reflect() reflects on the task result and updates its status.
    - reflect_and_complete() generates a final summary output after all tasks are done.
- Graph construction: create_agent_graph() builds the workflow graph with nodes and conditional edges.

"""




""" Agent states and LLM output schemas """

class TaskSchema(BaseModel):
    """ Schema by which an LLM can generate a single task """
    id: int = Field(description="Unique identifier for the task") # We shall see if the id thing actually works
    title: str = Field(description="Title of the task")
    description: str = Field(description="Brief description of the task")

class TodoListSchema(BaseModel):
    """ Schema for a list of tasks """
    tasks: list[TaskSchema] = Field(description="List of tasks generated from the goal")

class Task(TypedDict):
    """
    Represents a single, executable task in the agent workflow.
    Attributes: TODO
    """
    id: int
    title: str
    description: str
    status: Literal["pending", "complete", "failed", "needs-follow-up"]
    result: str | None

class AgentState(TypedDict):
    """
    Represents the overall state of the agent.
    Attributes:
        goal: The main objective the agent is trying to achieve, as entered by the user
        mode: Decides whether the agent waits for user confirmation before executing tasks ("confirm") or proceeds automatically ("auto")
        tasks: To-do list generated by LLM
        conversation_history: LLM message history maintained across all tasks for context
    """
    goal: str
    mode: Literal["confirm", "auto"]
    tasks: list[Task] | None
    current_task_id: int | None
    approved: bool
    conversation_history: list[str]  # Memory across tasks
    output: str | None  # Final output after all tasks are done





""" Agent nodes """

def generate_todos(state: AgentState) -> AgentState:
    """
    Generate TO-DO list from user goal using LLM.

    Takes the goal from the state, prompts the LLM to break it down into actionable tasks, 
    and adds them to the state with 'pending' status.
    
    """
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
    structured_llm = llm.with_structured_output(TodoListSchema)
    
    # LLM prompt
    prompt = f"""Create the simplest possible to-do list for this goal by breaking it down into 3-7 simple, actionable tasks:
    Goal: {state['goal']}
    
    Each task should represent a single, simple step.
    Each task should be achievable using only simple file operation tools: 'read file', 'write to file', 'append to file' or simple web search.
    
    Avoid steps such as 'record', 'confirm', or 'reflect' unless absolutely necessary.
    """

    response = structured_llm.invoke(prompt)

    # Print generated tasks
    print("\n" + "=" * 50)
    print("TO-DO LIST")
    print("=" * 50)

    for task in response.tasks:
        print(f"[ ] {task.title} - {task.description}")

    
    if state['mode'] == "auto":
        print("\nLet's get to work. Starting execution...")


    # Update state tasks and conversation history
    state["tasks"] = [
        {
            "id": task.id,
            "title": task.title,
            "description": task.description,
            "status": "pending",
            "result": None,
            "reflection": None  
        }
        for task in response.tasks 
    ]
    state["conversation_history"] = [f"Goal: {state['goal']}"]  # TODO: Move this to a better place
    
    return state


def display_and_wait_for_approval(state: AgentState) -> AgentState:
    """
    Display the generated TO-DO list and wait for user approval.
    This node is only used in 'confirm' mode.
    """
    print("\n" + "=" * 50)
    print("\nDo you approve this task list?")
    print("  [y] Yes - Start execution")
    print("  [n] No - Cancel")
    
    choice = input("\nYour choice: ").strip().lower()
    
    if choice == 'y' or choice == 'yes':
        state["approved"] = True
        print("\n Task list approved. Starting execution...\n")
    else:
        state["approved"] = False
        print("\n Task list rejected. Exiting...")
    
    return state


def select_next_task(state: AgentState) -> AgentState:
    """ Find and select the next pending task. First pending task is selected. """

    if state["tasks"] is not None:
        for task in state["tasks"]:
            if task["status"] == "pending":
                state["current_task_id"] = task["id"]
                return state
    
    # No pending tasks found - clear current_task_id
    state["current_task_id"] = None
    return state


def execute_task(state: AgentState) -> AgentState:
    """ 
    Execute the current task using available tools.
    Logs result in conversation history.
    """
      
    # Get next task 
    current_task = next((t for t in state["tasks"] if t["id"] == state["current_task_id"]), None)
    
    # Add task to conversation history
    state["conversation_history"].append(f"Executing task #{current_task['id']}: {current_task['title']}")
    print(f"\nTASK #{current_task['id']}: {current_task['title']}\n")
    
    # Initialize LLM with tools
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
    llm_with_tools = llm.bind_tools(AVAILABLE_TOOLS)
    
    # Create LLM prompt
    task_prompt = f"""Execute this task described with the title and description:
    Title: {current_task['title']}
    Description: {current_task['description']}
    Context: {state['conversation_history']}
    
    Focus on the current task. If any tasks failed previously, do not try to solve them.
    Use the available tools as needed to complete the task.
    Do not create files unless absolutely necessary.
    Put all created files in an 'agent-files/' directory."""
    
    response = llm_with_tools.invoke(task_prompt)

    if response.tool_calls:
        current_task['result'] = ''  # Initialize result once before processing all tool calls
        
        for tool_call in response.tool_calls:
            tool_name = tool_call['name']
            tool_args = tool_call['args']
        
            tool_func = next((t for t in AVAILABLE_TOOLS if t.name == tool_name), None)
            if tool_func:
                try:
                    result = tool_func.invoke(tool_args)
                    # Check if tool_args string representation is too long to log
                    if len(str(tool_args)) > 100: # Avoid logging large content in conversation history
                        state["conversation_history"].append(f"Tool '{tool_name}' executed.")
                        print(f"Tool '{tool_name}' executed.\n")
                    else:
                        state["conversation_history"].append(f"Tool '{tool_name}' executed with arguments {tool_args}")
                        print(f"Tool '{tool_name}' executed with arguments {tool_args}")
                    current_task['result'] += str(result)
                    state["conversation_history"].append(f"Result: {current_task['result']}") # TODO: Update for case where result is None, for instance tool call creates a file. (Maybe they should all return strings)
                    print(f"Result: {current_task['result']}\n")
                except Exception as e:
                    current_task['result'] += f"Tool execution failed: {e}"
                    state["conversation_history"].append(f"Tool {tool_name}execution failed: {e}")
                    print(f"Tool {tool_name} execution failed: {e}") # TODO: Maybe this should be added to conversation history? 

            else:
                current_task['result'] += f"Tool '{tool_name}' not found"
                state["conversation_history"].append(f"Tool '{tool_name}' not found in AVAILABLE_TOOLS")
                print(f"Tool '{tool_name}' not found in AVAILABLE_TOOLS")

    # No tool calls made
    else:
        result = response.content
        current_task['result'] = result
        state["conversation_history"].append(f"LLM Response: {result}")
        print(f"LLM Response: {result}\n")

    return state



def reflect(state: AgentState) -> AgentState:
    """ 
    Reflect on the output of a task and decide on the task's status 
    TODO: Involve human-in-the-loop when LLM deems it necessary
    """
    # Get the current task by ID
    current_task = next((t for t in state["tasks"] if t["id"] == state["current_task_id"]), None)
    
    if not current_task:
        print(f"Error: Could not find task with ID {state['current_task_id']}")
        return state
    
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)

    reflection_prompt = f"""Summarize the result of the recently completed task titled '{current_task['title']}'.
    Based on the result, choose one label for the task: "successful", "failed", or "needs follow-up".
    In no more than three sentences, briefly explain your decision. Be concise.
    This is the result: {current_task['result']}
    """

    response = llm.invoke(reflection_prompt)
    reflection = response.content

    # Determine status based on which keyword appears first in the reflection
    reflection_lower = reflection.lower()
    
    # Find positions of each keyword (returns -1 if not found)
    pos_successful = reflection_lower.find("successful")
    pos_success = reflection_lower.find("success")
    pos_failed = reflection_lower.find("failed")
    pos_follow_up = reflection_lower.find("follow-up")
    
    # Get the earliest success-related position
    success_positions = [p for p in [pos_successful, pos_success] if p != -1]
    earliest_success = min(success_positions) if success_positions else float('inf')
    
    # Get the earliest position among all keywords
    positions = {
        'complete': earliest_success,
        'failed': pos_failed if pos_failed != -1 else float('inf'),
        'needs-follow-up': pos_follow_up if pos_follow_up != -1 else float('inf')
    }
    
    # Find which keyword appears first
    first_status = min(positions.items(), key=lambda x: x[1])
    
    if first_status[1] != float('inf'):
        current_task['status'] = first_status[0]
    else:
        current_task['status'] = "complete"  # Default to complete if no keywords found

    current_task['reflection'] = reflection
    state["conversation_history"].append(f"Reflection on task #{state['current_task_id']}: {reflection}. Task marked as {current_task['status']}.")
    print(f"Reflection: {reflection}")
    print(f"✓ Task #{state['current_task_id']} marked as: {current_task['status']}\n")
    
    # Debug: show all task statuses
    print("Current task statuses:")
    for task in state["tasks"]:
        status_icon = "✓" if task["status"] == "complete" else "." if task["status"] == "pending" else "✗"
        print(f"  {status_icon} Task {task['id']}: {task['status']}")
    print()
    
    return state 



def reflect_and_complete(state: AgentState) -> AgentState:
    """ Mark the agent as having completed all tasks. """

    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)

    final_output_prompt = f"""The agent has completed all tasks for the goal: {state['goal']}.
    Based on the conversation history: {state['conversation_history']}, provide a concise summary of
    the final output or result achieved by the agent. If the goal was to answer a question, provide the answer."""

    response = llm.invoke(final_output_prompt)
    state["output"] = response.content

    return state




""" Conditional routing functions """

def should_wait_for_approval(state: AgentState) -> str:
    """
    Check if we need to wait for user approval based on mode.
    
    Returns:
        "wait" if in confirm mode, "execute" if in auto mode
    """
    if state["mode"] == "confirm":
        return "wait"
    return "execute"


def is_approved(state: AgentState) -> str:
    """
    Check if the user approved the task list.
    
    Returns:
        "execute" if approved, "end" if not approved
    """
    if state["approved"]:
        return "execute"
    return "end"


def has_more_tasks(state: AgentState) -> str:
    """
    Check if there are more pending tasks to execute.
    
    Returns:
        "execute" if there are pending tasks, "end" otherwise
    """
    if state["tasks"] is not None:
        pending_count = sum(1 for task in state["tasks"] if task["status"] == "pending")
        if pending_count > 0:
            print(f"→ {pending_count} pending task(s) remaining")
            return "execute"
    
    print("→ No more pending tasks. Moving to completion.")
    return "end"




""" Graph construction """

def create_agent_graph():
    """
    Create and compile the agent workflow graph.
    
    The graph consists of the following nodes:
    - generate_todos: Generate task list from user goal
    - select_next_task: Select the next pending task
    - execute_task: Execute the current task using LLM with tools
    - reflect_and_complete: Generate final summary after all tasks are done
    
    Returns:
        Compiled LangGraph application ready for execution
    """
    
    # Initialize StateGraph with AgentState schema
    workflow = StateGraph(AgentState)
    
    # Add nodes to the graph
    workflow.add_node("generate_todos", generate_todos)
    workflow.add_node("display_and_wait_for_approval", display_and_wait_for_approval)
    workflow.add_node("select_next_task", select_next_task)
    workflow.add_node("execute_task", execute_task)
    workflow.add_node("reflect", reflect)
    workflow.add_node("reflect_and_complete", reflect_and_complete)
    
    # Set entry point
    workflow.set_entry_point("generate_todos")
    
    # After generating todos, check if we need approval
    workflow.add_conditional_edges(
        "generate_todos",
        should_wait_for_approval,
        {
            "wait": "display_and_wait_for_approval",
            "execute": "select_next_task"
        }
    )
    
    # After waiting for approval, check if approved
    workflow.add_conditional_edges(
        "display_and_wait_for_approval",
        is_approved,
        {
            "execute": "select_next_task",
            "end": END
        }
    )

    # After selecting a task, execute it
    workflow.add_edge("select_next_task", "execute_task")

    # After executing a task, reflect on it
    workflow.add_edge("execute_task", "reflect")
    
    # After reflecting on the status of a task, check if there are more tasks to execute.
    # If there are, select next task.
    # If there aren't, reflect and complete.
    workflow.add_conditional_edges(
        "reflect",
        has_more_tasks,
        {
            "execute": "select_next_task",
            "end": "reflect_and_complete"
        }
    )

    # After reflection, end the workflow
    workflow.add_edge("reflect_and_complete", END)
    
    # Compile the graph with memory for state persistence
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app




""" Graph visualization """

def visualize_graph(output_path: str = "agent_workflow.png"):
    """
    Generate a visualization of the agent workflow graph.
    
    Args:
        output_path (str): Path where the PNG image will be saved
    
    Returns:
        str: Path to the generated image file
    """
    try:
        app = create_agent_graph()
        
        # Get the graph visualization as PNG
        png_data = app.get_graph().draw_mermaid_png()
        
        # Save to file
        with open(output_path, 'wb') as f:
            f.write(png_data)
        
        print(f"✓ Graph visualization saved to: {output_path}")
        return output_path
    
    except Exception as e:
        print(f"Error generating visualization: {type(e).__name__}: {str(e)}")
        print("Note: You may need to install additional dependencies:")
        print("  pip install pygraphviz")
        print("  or use the ASCII version with print_graph_ascii()")
        return None


