from typing import TypedDict, Literal
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from tools import *
from tools import AVAILABLE_TOOLS
import os

# Load environment variables from .env file
load_dotenv()

"""
Agent-driven todo executor - Implementation with LangGraph

This module implements a LangGraph AI agent that takes a high-level goal, generates a todo list,
and executes tasks autonomously or with user confirmation.

Key Components:
- Task and AgentState: TypedDict schemas for state management. AgentState is the main state object, Task is a single task in the todo list.
- Node functions: generate_todos
- Graph construction

Usage: TODO
"""


""" Agent states and LLM output schemas """

class TaskSchema(BaseModel):
    """ Schema by which an LLM can generate a single task """
    id: int = Field(description="Unique identifier for the task") # We shall see if the id thing actually works
    title: str = Field(description="Title of the task")
    description: str = Field(description="Brief description of the task")

class TodoListSchema(BaseModel):
    """ Schema for a list of tasks """
    tasks: list[TaskSchema] = Field(description="List of tasks generated from the goal")

class Task(TypedDict):
    """
    Represents a single, executable task in the agent workflow.
    Attributes: TODO
    """
    id: int
    title: str
    description: str
    status: Literal["pending", "complete", "failed", "needs-follow-up"]
    result: str | None

class AgentState(TypedDict):
    """
    Represents the overall state of the agent.
    Attributes:
        goal: The main objective the agent is trying to achieve, as entered by the user
        mode: Decides whether the agent waits for user confirmation before executing tasks ("confirm") or proceeds automatically ("auto")
        tasks: To-do list generated by LLM
        conversation_history: LLM message history maintained across all tasks for context
    """
    goal: str
    mode: Literal["confirm", "auto"]
    tasks: list[Task] | None
    current_task_id: int | None
    approved: bool
    user_action: str | None  # approve, edit, regenerate, cancel
    conversation_history: list[str]  # Memory across tasks
    output: str | None  # Final output after all tasks are done





""" Agent nodes """

def generate_todos(state: AgentState) -> AgentState:
    """
    Generate TO-DO list from user goal using LLM.

    Takes the goal from the state, prompts the LLM to break it down into actionable tasks, 
    and adds them to the state with 'pending' status.
    
    """
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
    structured_llm = llm.with_structured_output(TodoListSchema)
    
    # LLM prompt
    prompt = f"""Break down this goal into 3-7 actionable tasks:
    Goal: {state['goal']}
    
    Each task should be concrete and executable."""

    response = structured_llm.invoke(prompt)

    # Print generated tasks
    print("\n" + "=" * 50)
    print("TO-DO LIST")
    print("=" * 50)

    for task in response.tasks:
        print(f"[ ] {task.title} - {task.description}")

    print("\nTo-Do list generated! Let's start working...")

    print("\n" + "=" * 50)
    print("WORKFLOW")
    print("=" * 50)

    # Update state tasks and conversation history
    state["tasks"] = [
        {
            "id": task.id,
            "title": task.title,
            "description": task.description,
            "status": "pending",
            "result": None,
            "reflection": None  
        }
        for task in response.tasks 
    ]
    state["conversation_history"] = [f"Goal: {state['goal']}"]  # TODO: Move this to a better place
    
    return state


def select_next_task(state: AgentState) -> AgentState:
    """ Find and select the next pending task. First pending task is selected. """

    if state["tasks"] is not None: # TODO: Handle case where tasks is None
        for task in state["tasks"]:
            if task["status"] == "pending":
                state["current_task_id"] = task["id"]
                return state


def execute_task(state: AgentState) -> AgentState:
    """ 
    Execute the current task using available tools.
    """

    task_failed = False # Keeps track of whether the task has failed at some point
      
    # TODO: Handle cases where current_task_id is None or current_task is None

    # Get next task 
    current_task = next((t for t in state["tasks"] if t["id"] == state["current_task_id"]), None)
    
    # Add task to conversation history
    state["conversation_history"].append(f"Executing task #{current_task['id']}: {current_task['title']}")
    print(f"\nTASK #{current_task['id']}: {current_task['title']}\n")
    
    # Initialize LLM with tools
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
    llm_with_tools = llm.bind_tools(AVAILABLE_TOOLS)
    
    # Create LLM prompt
    task_prompt = f"""Execute this task:
    Title: {current_task['title']}
    Description: {current_task['description']}
    Context: {state['conversation_history']}
    
    Use the available tools as needed to complete the task."""
    
    response = llm_with_tools.invoke(task_prompt)

    if response.tool_calls:
        tool_call = response.tool_calls[0] # TODO: Extend to case where you have multiple tool calls
        tool_name = tool_call['name']
        tool_args = tool_call['args']
        
        tool_func = next((t for t in AVAILABLE_TOOLS if t.name == tool_name), None)
        if tool_func:
            try:
                result = tool_func.invoke(tool_args)
                state["conversation_history"].append(f"Tool '{tool_name}' executed with arguments {tool_args}")
                print(f"Tool '{tool_name}' executed with arguments {tool_args}")
                current_task['result'] = str(result)
                state["conversation_history"].append(f"Result: {current_task['result']}") # TODO: Update for case where result is None, for instance tool call creates a file. (Maybe they should all return strings)
                print(f"Result: {current_task['result']}\n")
                current_task['status'] = "complete"
            except Exception as e:
                print(f"Tool execution failed: {e}") # TODO: Maybe this should be added to conversation history? 
                task_failed = True
        else:
            print(f"Tool '{tool_name}' not found in AVAILABLE_TOOLS")
            task_failed = True

    # No tool calls made
    else:
        result = response.content
        state["conversation_history"].append(f"LLM Response: {result}")
        print(f"LLM Response: {result}\n")
        current_task['status'] = "complete"


    if task_failed:
        current_task['status'] = "failed" # TODO: Involve human-in-the-loop

    return state


def reflect_and_complete(state: AgentState) -> AgentState:
    """ Mark the agent as having completed all tasks. """

    
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)

    final_output_prompt = f"""The agent has completed all tasks for the goal: {state['goal']}.
    Based on the conversation history: {state['conversation_history']}, provide a concise summary of
    the final output or result achieved by the agent. If the goal was to answer a question, provide the answer."""

    response = llm.invoke(final_output_prompt)
    state["output"] = response.content

    return state




""" Conditional routing function """

def has_more_tasks(state: AgentState) -> str:
    """
    Check if there are more pending tasks to execute.
    
    Returns:
        "execute" if there are pending tasks, "end" otherwise
    """
    if state["tasks"] is not None:
        for task in state["tasks"]:
            if task["status"] == "pending":
                return "execute"
    return "end"




""" Graph construction """

def create_agent_graph():
    """
    Create and compile the agent workflow graph.
    
    The graph consists of the following nodes:
    - generate_todos: Generate task list from user goal
    - select_next_task: Select the next pending task
    - execute_task: Execute the current task using LLM with tools
    - reflect_and_complete: Generate final summary after all tasks are done
    
    Returns:
        Compiled LangGraph application ready for execution
    """
    
    # Initialize StateGraph with AgentState schema
    workflow = StateGraph(AgentState)
    
    # Add nodes to the graph
    workflow.add_node("generate_todos", generate_todos)
    workflow.add_node("select_next_task", select_next_task)
    workflow.add_node("execute_task", execute_task)
    workflow.add_node("reflect_and_complete", reflect_and_complete)
    
    # Set entry point
    workflow.set_entry_point("generate_todos")
    
    # After generating todos, select the first task
    workflow.add_edge("generate_todos", "select_next_task")

    # After selecting a task, execute it
    workflow.add_edge("select_next_task", "execute_task")
    
    # After executing a task, check if there are more tasks to execute.
    # If there are, select next task.
    # If there aren't, reflect and complete.
    workflow.add_conditional_edges(
        "execute_task",
        has_more_tasks,
        {
            "execute": "select_next_task",
            "end": "reflect_and_complete"
        }
    )

    # After reflection, end the workflow
    workflow.add_edge("reflect_and_complete", END)
    
    # Compile the graph with memory for state persistence
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app






